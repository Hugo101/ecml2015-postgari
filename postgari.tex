\documentclass[final]{beamer}

\usetheme{enziteto}

\usepackage[orientation=portrait, size=a0, scale=1.4, debug]{beamerposter}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[absolute, showboxes, overlay]{textpos}
%\usepackage[absolute, overlay]{textpos}
\usepackage{calc}
\usepackage[colorgrid,texcoord]{eso-pic}
\usepackage{biblatex}
% \usepackage{enumitem}

\addbibresource{../referomnia/referomnia.bib}

\usepackage{blindtext}


\newcommand*\mccol[2]{\multicolumn{#1}{c}{#2}}
\newcommand*\tmccol[2]{\mccol{#1}{\tiny\textsf{#2}}}
\newcommand*\bmccol[2]{\mccol{#1}{\textbf{#2}}}


% \usepackage{xparse}
% \ExplSyntaxOn
% \NewDocumentCommand{\convertto}{mm}
% % #1 = em or ex (or any other unit)
% % #2 = dimen to convert
% {
%   \texttt{#2~=~\fp_to_decimal:n { round ( #2/1#1, 5 ) }#1}
% }
% \DeclareExpandableDocumentCommand{\thelength}{ O{mm} m }
% {
%   \fp_to_decimal:n { round ( #2/1#1, 5 ) } #1
% }
% \ExplSyntaxOff


% 
% custom colors
\definecolor{untractable_red}{RGB}{209, 25, 25}
\definecolor{tractable_green}{RGB}{0, 153, 51}


\setbeamertemplate{itemize item}{\raisebox{.21ex}{\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\oplus}$}}\hspace{0pt}}}
\setbeamertemplate{itemize subitem}{\raise .2ex\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\otimes}$}}\hspace{0pt}}
\setbeamertemplate{itemize subsubitem}{\textcolor{lacamlilac}{$\oplus$}}
\setbeamertemplate{bibliography item}{\hspace{10pt}\raise .2ex\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\oplus}$}}}


% \setbeamertemplate{headline}{}

% \addbibresource{../referomnia/referomnia.bib}

\title{Simplifying, Regularizing and Strengthening Sum-Product Network Structure Learning}
\author{Antonio  Vergari, Nicola  {Di Mauro} and Floriana Esposito}
\date{}


\begin{document}

\institute{Universit√† degli Studi di Bari}
\department{Dipartimento di Informatica}
\laboratory{LACAM}
\group{Machine Learning}
\institutelogo{\includegraphics[width=25pt]{figures/unibaba}}
\lablogo{\includegraphics[width=35pt]{figures/lacam}}


% {
%   \setbeamertemplate{headline}{}
%   \setbeamertemplate{footline}{}
%   \begin{textblock}
%     \titlepage
%   \end{textblock}
% }

\newcommand{\hmargin}{20mm}
\newcommand{\vmargin}{20mm}
\textblockorigin{\hmargin}{\vmargin}

\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}

%
% TODO: generalize this
\newlength{\posterwidth}
\setlength{\posterwidth}{841mm - 2\hmargin}
\newlength{\posterheight}
\setlength{\posterheight}{1189mm}

\newcommand{\ncols}{3}
\newlength{\colwidth}
\setlength{\colwidth}{\posterwidth/\ncols}


\newlength{\colhpoint}


\begin{frame}{}
  %
  % title
  % \textblockcolour{header}
  \begin{textblock}{58}(0, 0)
    \usebeamerfont{section name}
    \huge
    Simplifying, Regularizing and Strengthening\\
    Sum-Product Network Structure Learning
  \end{textblock}
  %
  % authors
  \begin{textblock}{30}(0, 6.5)
    \usebeamerfont{author}
    \small
    Antonio  Vergari, Nicola  {Di Mauro} and Floriana Esposito
  \end{textblock}
  % 
  % email
  \begin{textblock}{15}(30, 6.5)
    \usebeamerfont{author}
    \small
    \emph{\{firstname.lastname@uniba.it\}}
  \end{textblock}
  %
  % affiliations
  \begin{textblock}{20}(60, 0)
    \usebeamerfont{author}
    \scriptsize
    \begin{minipage}[t]{5cm}
      \vspace{0pt}\hspace{10pt}
      \includegraphics[width=90pt]{figures/unibaba}
    \end{minipage}
    \begin{minipage}[t]{12cm}
    \vspace{27pt}
      \flushleft
      University of Bari "Aldo Moro", Italy\\
    \vspace{2pt}
      Department of Computer Science
    \end{minipage}\\[0.75cm]
    \usebeamerfont{author}
    \scriptsize
    \begin{minipage}[t]{5cm}
      \vspace{0pt}
      \includegraphics[width=110pt]{figures/lacam}
    \end{minipage}
    \begin{minipage}[t]{12cm}
      \vspace{23pt}
      \flushleft
      LACAM\\
      \vspace{2pt}
      Machine Learning
    \end{minipage}
  \end{textblock}
  
  
  %
  % section 1
  \begin{textblock}{80}(0, 9.8)
    \usebeamerfont{section name}
    Sum-Product Networks and Tractable Models
  \end{textblock}
  
  
  \begin{textblock}{25.7}(0, 12.3)
    \footnotesize
    Probabilistic Graphical Models (PGMs) provide a tool to compactly
    represent joint probability distributions $P(\mathbf{X})$.

    However, \emph{\textbf{inference}}, the main task one may want to perform on a PGM, is generally \emph{\textbf{untractable}}.\bigskip  
    \begin{table}[!ht]
      \setlength{\tabcolsep}{10pt}
      \centering
      \begin{tabular}{c c c c}
        \scriptsize\color{untractable_red}  \textbf{\emph{untractable}} & \scriptsize\color{untractable_red} \textbf{\emph{untractable}}& \scriptsize\color{tractable_green} \emph{\textbf{tractable}} & \scriptsize\color{tractable_green} \emph{\textbf{tractable}}\\

        
        \includegraphics[width=0.22\linewidth]{figures/mrf} &
        \includegraphics[width=0.22\linewidth]{figures/bn} &
        \includegraphics[width=0.22\linewidth]{figures/clt} &
        \includegraphics[width=0.22\linewidth]{figures/nf}\\                                                             
        \addlinespace[-0.2cm]
        % \scriptsize  $P(\mathbf{X})=\frac{1}{Z}\prod_{c}\phi_{c}(\mathbf{X}_{c})$ & 
        % \scriptsize  $P(\mathbf{X})=\prod_{i=1}^nP(X_{i}|\mathbf{Pa}_{i})$&
        %                                                                  %           \addlinespace[0.5cm]
                                                                            
        %                                                                     %\addlinespace[-0.2cm]
        % \scriptsize $P(\mathbf{X})=\prod_{i=1}^nP(X_{i}|Pa_{i})$ &
        % \scriptsize $P(\mathbf{X})=\prod_{i=1}^nP(X_{i})$ \\                                                             
        \tiny  $P(\mathbf{X})=\frac{1}{Z}\prod\limits_{c}\phi_{c}(\mathbf{X}_{c})$ & 
        \tiny  $P(\mathbf{X})=\prod\limits_{i=1}^nP(X_{i}|\mathbf{Pa}_{i})$&
        % \addlinespace[0.5cm]
        \tiny $P(\mathbf{X})=\prod\limits_{i=1}^nP(X_{i}|Pa_{i})$ &
        \tiny $P(\mathbf{X})=\prod\limits_{i=1}^nP(X_{i})$                                                              
      \end{tabular}\bigskip
      
    \end{table}

    To guarantee polynomial inference, tractable models trade off
    model expressiveness.
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 12.3)
    \footnotesize
    Sum-Product Networks (SPNs) are DAGs
    \emph{compiling} a pdf $P(\mathbf{X})$ into a \textbf{\emph{deep}} architecture of \textbf{sum}
    and \textbf{product} nodes over univariate distributions $X_1,\dots,X_n$ as leaves.
    The parameters of the network are the weights $w_{ij}$ associated to sum
    nodes children edges.\par\bigskip
    
    Product nodes define factorizations over independent vars, sum
    nodes mixtures.\par\bigskip
    
    % Sum node children weights are the parameters of the model.\\
    
    Products over nodes with different scopes (\emph{decomposability}) and
    sums over nodes with same scopes (\emph{completeness}) guarantee modeling
    a pdf (\emph{validity}).\par\bigskip
    
    
    \begin{minipage}{0.5\linewidth}
      \centering
      \includegraphics[width=0.65\linewidth]{figures/spn-prod}
    \end{minipage}\begin{minipage}{0.5\linewidth}
      \centering
      \includegraphics[width=0.55\linewidth]{figures/spn-sum}
    \end{minipage}
    
    
  \end{textblock}

  % \begin{textblock}{25.7}(34.1, 15.3)
  %   \small
  %     \includegraphics[width=0.7\linewidth]{figures/spn-sum}
  % \end{textblock}
  
  \begin{textblock}{25.7}(54.2, 12.3)
    \footnotesize
    \begin{minipage}{0.7\linewidth}
    
    Bottom-up evaluation of the network:
    $$S_{X_i}(x_j)=P(X_i=x_j)$$    
    $$S_{+}(\mathbf{x})=\sum\limits_{i\in
      ch(+)}w_{i}S_{i}(\mathbf{x})\quad S_{\times}(\mathbf{x})=\prod\limits_{i\in
      ch(\times)}S_{i}(\mathbf{x})$$\\[10pt]
    \setlength{\leftmargini}{30pt}
    Inferences linear in the \emph{\textbf{size of the network}} (\emph{\# edges}):
    \begin{itemize}
    \item $Z = S(*)$ (all leaves output 1)
    \item $P(\mathbf{e}) = S(\mathbf{e})/S(*)$
    \item $P(\mathbf{q}| \mathbf{e}) = \frac{P(\mathbf{q},
        \mathbf{e})}{P(\mathbf{e})} = \frac{S(\mathbf{q},
        \mathbf{e})}{S(\mathbf{e})}$
    \item $MPE(\mathbf{q},\mathbf{e}) = \max_{\mathbf{q}}P(\mathbf{q},
      \mathbf{e}) = S^{max}(\mathbf{e})$, turning sum nodes into max nodes
    \end{itemize}
  \end{minipage}\begin{minipage}{0.28\linewidth}
    \includegraphics[width=0.8\linewidth]{figures/spn-long}
  \end{minipage}\\[20pt]

  The \emph{\textbf{depth of the network}} (\emph{\# layers})
  determines expressive efficiency~\parencite{Martens2014,Zhao2015}

  \end{textblock}
  
  %
  % section 2
  \begin{textblock}{80}(0, 28.5)
    \usebeamerfont{section name}
    How and why to perform structure learning
    
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 31.)
    \footnotesize
     
    SPN structure learning is a constraint-based
    search. Main ideas: to discover hidden variables for sum nodes and independences
    for product nodes by applying some form of clustering
    along matrix axis. Different variations:
    using K-Means on
    features~\emph{\parencite{Dennis2012}}; merging features bottom-up
    with IB heuristics\emph{~\parencite{Peharz2013}};
    \emph{\textbf{LearnSPN}}~\emph{\parencite{Gens2013}} is the first principled top-down greedy
    algorithm. 
    

    % \begin{itemize}
    %   \itemsep 6pt
    % \item greedy top-down: KMeans on features~\emph{\parencite{Dennis2012}};
      
    % \item greedy bottom up: merging feature regions by a \emph{Bayesian-Dirichlet independence test},  and reducing edges by maximizing MI\emph{~\parencite{Peharz2013}}

    % \item \textbf{ID-SPN}: turning LearnSPN in log-likelihood guided expansion of sub-networks
    %   approximated by Arithmetic Circuits~\emph{\parencite{Rooshenas2014}}
      
    % \end{itemize}
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 31.)
    \footnotesize
    LearnSPN builds a tree-like SPN by recursively split the data
    matrix.  It splits columns in pairs by a greedy \textbf{\emph{G Test}} based
    procedure with threshold $\rho$: $G(X_i, X_j) =  2\sum_{x_i \sim
      X_i}\sum_{x_j \sim X_j}c(x_i, x_j)\cdot \log\frac{c(x_i,
      x_j)\cdot |T|}{c(x_i)c(x_j)}$ (Figure 1.c); it clusters instances in
    $|C|$ sets with \textbf{\emph{online Hard-EM}} (Figure 1.b) with cluster number penalty
    $\lambda$: $Pr(\mathbf{X})= \sum_{C_i \in \mathbf{C}}\prod_{X_j
      \in \mathbf{X}}Pr(X_j,C_i)$. Weights are the cluster proportions.

    % \begin{itemize}
    % \item splitting columns in pairs by a greedy \textbf{\emph{G Test}} based
    %   procedure with threshold $\rho$:
    %   \[
    %   G(X_i, X_j) =  2\sum_{x_i \sim X_i}\sum_{x_j \sim X_j}c(x_i, x_j)\cdot \log\frac{c(x_i, x_j)\cdot |T|}{c(x_i)c(x_j)}
    %   \]
 
    %   \end{itemize}
  \end{textblock}
  
  
  \begin{textblock}{60}(0, 36.7)
    \small
    \begin{minipage}[t][][t]{5.67cm}
      \includegraphics[width=\linewidth]{figures/grid-0}
    \end{minipage}\hspace{30pt}\begin{minipage}[t]{4.986cm}
      \includegraphics[width=\linewidth]{figures/grid-1}
    \end{minipage}\hspace{30pt}\raisebox{82pt}{\begin{minipage}[t]{5.67cm}\vspace{-120pt}
      \includegraphics[width=\linewidth]{figures/learnspn-1}
    \end{minipage}}\hspace{30pt}\begin{minipage}[t]{4.986cm}
      \includegraphics[width=\linewidth]{figures/grid-2}
    \end{minipage}\hspace{30pt}\raisebox{42pt}{\begin{minipage}[t]{6.48cm}
      \includegraphics[width=\linewidth]{figures/learnspn-2}
    \end{minipage}}\hspace{30pt}\begin{minipage}[t]{5.1516cm}
    \includegraphics[width=\linewidth]{figures/grid-3}                                                               \end{minipage}\hspace{30pt}\raisebox{42pt}{\begin{minipage}[t]{8.1cm}
      \includegraphics[width=\linewidth]{figures/learnspn-3}                                             
    \end{minipage}}\hspace{30pt}\raisebox{210pt}{\begin{minipage}[t]{4cm}
      \tiny\flushleft
      Figure 1.\\
      LearnSPN steps depiction starting from a full data matrix \emph{(a)},
      clustering on rows \emph{(b)}, then on columns \emph{(c)}, and putting a naive
      factorization on leaves \emph{(d)}
    \end{minipage}}\\
  \vspace{-20pt}\hspace{80pt}\begin{minipage}[t]{7cm}
    \scriptsize\emph{(a)}
  \end{minipage}\hspace{80pt}\begin{minipage}[t]{7cm}
    \scriptsize\emph{(b)}
  \end{minipage}\hspace{175pt}\begin{minipage}[t]{7cm}
    \scriptsize\emph{(c)}
  \end{minipage}\hspace{175pt}\begin{minipage}[t]{7cm}
    \scriptsize\emph{(d)}
  \end{minipage}
    
    % \begin{table}[ht]
    %   \centering
    %   \begin{tabular}{l l l l l l l}
    %     \includegraphics[width=0.228\linewidth]{figures/grid-0}&
    %     \includegraphics[width=0.2\linewidth]{figures/grid-1}&
    %     \includegraphics[width=0.2\linewidth]{figures/learnspn-1}&                                                       \includegraphics[width=0.2\linewidth]{figures/grid-2}&
    %     \includegraphics[width=0.24\linewidth]{figures/learnspn-2}&
    %     \includegraphics[width=0.208\linewidth]{figures/grid-3}&                                                          \includegraphics[width=0.30\linewidth]{figures/learnspn-3}\\                                             
    %   \end{tabular}
    % \end{table}
    
  \end{textblock}
  
  
  \begin{textblock}{25.7}(54.2, 31.)
    \footnotesize
    If there are less than $m$ instances, it puts a \textbf{\emph{naive
        factorization}} over leaves (Figure 1.d). For each univariate distribution
    it gets its \emph{\textbf{ML estimation}} smoothed by $\alpha$. LearnSPN
    hyperparameter space is thus: $\{\rho, \lambda, m, \alpha\}$.\par\bigskip

   %  \small
  %   \begin{itemize}
  % \item clustering instances with \textbf{\emph{online Hard-EM}} with cluster penalty
  %   $\lambda$:
  %   \[\begin{array}{cc}
  %       Pr(\mathbf{X})= \sum_{C_i \in \mathbf{C}}\prod_{X_j \in \mathbf{X}}Pr(X_j|C_i)Pr(C_i)\\
  %       % & Pr(C_i) \propto e^{-\lambda |\mathbf{C}|\cdot |\mathbf{X}|}\\
  %     \end{array}\]
  %     weights are the proportions of instances falling into each cluster
  %   \item if there are less than $m$ instances, put a \textbf{\emph{naive
  %         factorization}} over leaves
  %   \item each univariate distribution get \emph{\textbf{ML
  %         estimation}} smoothed by $\alpha$  
  %   \end{itemize}

    The state-of-the-art, in terms of test likelihood, is \textbf{ID-SPN}: it turns LearnSPN in log-likelihood guided expansion of sub-networks
    approximated by Arithmetic
    Circuits~\emph{\parencite{Rooshenas2014}}. However it is
    overparametrized, and slower.\par\bigskip
    
    
    Tractability is guaranteed if the network size is polynomial in \#
    vars. \emph{\textbf{Structure quality matters}} as much as likelihood. comparing network sizes is more solid than comparing inference times.\par\bigskip

    LearnSPN is too greedy and the resulting SPNs are overcomplex
    networks that may not generalize well. \textbf{\emph{Structure quality desiderata}}: \emph{smaller} but \emph{accurate}, \emph{deeper} but not wider, SPNs. 

  \end{textblock}
  
  
  %
  % section 3
  \begin{textblock}{80}(0, 47.2)
    \usebeamerfont{section name}
    Simplifying by limiting node splits
  \end{textblock}

  % 
  % section 3.1
  \begin{textblock}{20}(54.2, 47.2)
    \usebeamerfont{section name}
    Experiments
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 49.7)
    \footnotesize
    \textsf{LearSPN} performs two interleaved \textbf{\emph{greedy
        hierarchical}} divisive \textbf{\emph{clustering}}
    processes (co-clustering). Each process benefits from the other one improvements/highly suffers
    from other's mistakes.\par\bigskip

    Idea: slowing down the processes by limiting the number of
    nodes to split into. \textsf{SPN-B}, variant of \textsf{LearnSPN} that uses EM
    for mixture modeling with
    $k=2$ to cluster rows.\par\bigskip

    Pros:
    \begin{itemize}
    \item not committing to complex structures too early  
    \item same expressive power: successive splits\\ allow for more node children
    \item reducing node out fan increases the depth
    \item same accuracy, smaller networks
    \end{itemize}
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 49.7)

    \footnotesize
    By increasingly limiting the max number of allowed splits the depth of the
    structures increases. It is also worth noting how the size of the
    network decreases. 
    Other
    
    \begin{table}[ht]
      \setlength{\tabcolsep}{30pt}  
      \centering
      \begin{tabular}{c c}
        \includegraphics[width=0.4\linewidth]{figures/nltcs-depth.pdf}&\includegraphics[width=0.4\linewidth]{figures/plants-depth.pdf}
      \end{tabular}
    \end{table}
  \end{textblock}
  
  \begin{textblock}{25.7}(54.2, 49.7)
    \footnotesize
    Classical setting for \emph{\textbf{generative}} graphical models
    structure learning \parencite{Gens2013}:
    \setlength{\leftmargini}{30pt}
    \begin{itemize}
      \itemsep 7pt
    \item 19 binary datasets from classification, recommendation,
      frequent pattern mining\dots \parencite{Lowd2010} \parencite{Haaren2012}
    \item Training 75\% Validation 10\% Test 15\%  splits (no cv)
    \end{itemize}\bigskip

    Comparing both accuracy and structure quality:
    \begin{itemize}
    \item \emph{\textbf{average log-likelihood}} on predicting test
      instances
    \item networks sizes (\# edges)
    \item network depth (\# alternated type layers)
     % \item latent interactions captured (\# number of parameters)

    \end{itemize}\bigskip
    
    Comparing the state-of-the-art, \textsf{LearnSPN},
    \textsf{ID-SPN} and \textsf{MT}~\parencite{Meila2000}, against our
    variations:
    
    \begin{itemize}
    \item \textsf{SPN-B} using only \textsf{B}inary splits
    \item \textsf{SPN-BT} with \textsf{B}inary splits and \textsf{T}rees as leaves
    \item \textsf{SPN-BB} combining \textsf{B}inary splits and \textsf{B}agging
      \item \textsf{SPN-BTB} including all variants
    \end{itemize}\bigskip
    
    Model selection via \textit{grid search} in the same parameter
    space:
    \begin{minipage}[t]{0.35\linewidth}
      \begin{itemize}
      \item $\lambda \in \{0.2, 0.4, 0.6, 0.8\}$,
      \item $\rho \in \{5, 10, 15, 20\}$, 
      \end{itemize}
    \end{minipage}\begin{minipage}[t]{0.5\linewidth}
      \begin{itemize}
      \item $m \in \{1, 50, 100, 500\}$, 
      \item $\alpha \in \{ 0.1, 0.2, 0.5, 1.0, 2.0\}$.
      \end{itemize}
    \end{minipage}

    \begin{table}[ht]
      \setlength{\tabcolsep}{30pt}  
      \centering
      \begin{tabular}{c c}
        \includegraphics[width=0.4\linewidth]{figures/edges-comp.pdf}&\includegraphics[width=0.4\linewidth]{figures/levels-comp.pdf}
      \end{tabular}
    \end{table}
  


      % \begin{table}[!htbp]
      %   \centering
      %   \tiny
      %   \setlength{\tabcolsep}{3pt}  
      %   \begin{tabular}{r r r r l r r r r r r}
      %     \toprule
      %     & \bmccol{4}{\# edges}    & \bmccol{3}{\# layers} & \bmccol{3}{\# params}                                                                                                                                                   \\
      %     \midrule
      %     % & \textsf{LearnSPN}       & \textsf{SPN-B}
      %     % & SPN-BT}                 & LearnSPN}             & SPN-B}                & SPN-BT}                 & LearnSPN}            & SPN-B}                & SPN-BT}                                                                \\
      %     & \tmccol{1}{LearnSPN}    & \tmccol{1}{SPN-B}     & \tmccol{2}{SPN-BT}    & \tmccol{1}{LearnSPN}    & \tmccol{1}{SPN-B}    & \tmccol{1}{SPN-BT}    & \tmccol{1}{LearnSPN}    & \tmccol{1}{SPN-B}    & \tmccol{1}{SPN-BT}    \\
      %     % & \rot{\textsf{LearnSPN}} & \rot{\textsf{SPN-B}}  & \rot{\textsf{SPN-BT}} & \rot{\textsf{LearnSPN}} & \rot{\textsf{SPN-B}} & \rot{\textsf{SPN-BT}} & \rot{\textsf{LearnSPN}} & \rot{\textsf{SPN-B}} & \rot{\textsf{SPN-BT}} \\
      %     % & e                       & l                     & p                     & e                       & l                    & p                     & e                       & l                    & p                     \\
      %     \midrule       
      %     \textbf{NLTCS}      & 7509                    & 1133                  & 1133                  & (1125)                  & 4                    & 15                    & 15                      & 476                  & 275  & 275            \\
      %     \textbf{MSNBC}      & 22350                   & 4258                  & 4258                  & (3996)                  & 4                    & 21                    & 21                      & 1680                 & 1071 & 1071           \\
      %     \textbf{KDDCup2k}   & 44544                   & 4272                  & 4272                  & (4166)                  & 4                    & 25                    & 25                      & 753                  & 760  & 760            \\
      %     \textbf{Plants}     & 55668                   & 13720                 & 5948                  & (1840)                  & 6                    & 23                    & 20                      & 3819                 & 2397 & 490            \\
      %     \textbf{Audio}      & 70036                   & 16421                 & 4059                  & (478)                   & 8                    & 23                    & 15                      & 3389                 & 2631 & 105            \\
      %     \textbf{Jester}     & 36528                   & 10793                 & 10793                 & (8587)                  & 4                    & 19                    & 19                      & 563                  & 1932 & 1932           \\
      %     \textbf{Netflix}    & 17742                   & 25009                 & 4132                  & (203)                   & 4                    & 25                    & 14                      & 1499                 & 4070 & 82             \\
      %     \textbf{Accidents}  & 48654                   & 12367                 & 10547                 & (6687)                  & 6                    & 25                    & 26                      & 5390                 & 2708 & 1977           \\
      %     \textbf{Retail}     & 7487                    & 1188                  & 1188                  & (1153)                  & 4                    & 23                    & 23                      & 171                  & 224  & 224            \\
      %     \textbf{Pumsb-star} & 15247                   & 12800                 & 9984                  & (6175)                  & 8                    & 25                    & 23                      & 1911                 & 2662 & 1680           \\
      %     \textbf{DNA}        & 17602                   & 3178                  & 4225                  & (2746)                  & 6                    & 13                    & 12                      & 947                  & 884  & 1113           \\
      %     \textbf{Kosarek}    & 7993                    & 8174                  & 2216                  & (1311)                  & 6                    & 27                    & 21                      & 781                  & 1462 & 242            \\
      %     \textbf{MSWeb}      & 17339                   & 9116                  & 7568                  & (6797)                  & 6                    & 27                    & 34                      & 620                  & 1672 & 1446           \\
      %     \textbf{Book}       & 42491                   & 9917                  & 3503                  & (3485)                  & 4                    & 15                    & 13                      & 1176                 & 1351 & 430            \\
      %     \textbf{EachMovie}  & 52693                   & 20756                 & 20756                 & (17861)                 & 8                    & 23                    & 23                      & 1010                 & 2637 & 2637           \\
      %     \textbf{WebKB}      & 52498                   & 45620                 & 8796                  & (6874)                  & 8                    & 23                    & 16                      & 1712                 & 6087 & 1128           \\
      %     \textbf{Reuters-52} & 307113                  & 77336                 & 77336                 & (59197)                 & 12                   & 31                    & 31                      & 3641                 & 8968 & 8968           \\
      %     % \textbf{20 NewsG} & 438100                  & 169326                & (114951)              & 10                      & 31                   & 33                    & 3442                    & 18117                & 18065                 \\
      %     \textbf{BBC}        & 318313                  & 63723                 & 63723                 & (41247)                 & 16                   & 27                    & 27                      & 1134                 & 6147 & 6147           \\
      %     \textbf{Ad}         & 70056                   & 23606                 & 23606                 & (20079)                 & 16                   & 59                    & 59                      & 1060                 & 1222 & 1222           \\
      %     \bottomrule
      %   \end{tabular}
      %   \caption[Experimentation statistics]{\scriptsize Structural quality results for
      %     the best validation models for \textsf{LearnSPN},
      %     \textsf{SPN-B} and \textsf{SPN-BT} as the number
      %     of edges, layers and parameters. For \textsf{SPN-BT} are reported
      %     the number of edges considering those in the Chow-Liu leaves
      %     and without considering them (in parenthesis).}
      %   \label{tab:expstats}
      % \end{table}



      \begin{table}[!htbp]
        \centering
        \scriptsize
        \setlength{\tabcolsep}{3pt}  
        \begin{tabular}{r r r r r r r r}
          \toprule
          & \textsf{LearnSPN} & \textsf{SPN-B} & \textsf{SPN-BT} & \textsf{ID-SPN}  & \textsf{SPN-BB}   & \textsf{SPN-BTB}  & \textsf{MT}      \\
          \midrule                                                                                     
          \textbf{NLTCS}      & -6.110            & -6.048         & -6.048          & \textbf{-5.998}  & -6.014            & -6.014            & -6.008           \\
          \textbf{MSNBC}      & -6.099            & -6.040         & -6.039          & -6.040           & \textbf{-6.032}   & -6.033            & -6.076           \\
          \textbf{KDDCup2k}   & -2.185            & -2.141         & -2.141          & -2.134           & -2.122            & \textbf{-2.121}   & -2.135           \\
          \textbf{Plants}     & -12.878           & -12.813        & -12.683         & -12.537          & -12.167           & \textbf{-12.089}  & -12.926          \\
          \textbf{Audio}      & -40.360           & -40.571        & -40.484         & -39.794          & -39.685           & \textbf{-39.616}  & -40.142          \\
          \textbf{Jester}     & -53.300           & -53.537        & -53.546         & \textbf{-52.858} & \textbf{-52.873}  & -53.600           & -53.057          \\
          \textbf{Netflix}    & -57.191           & -57.730        & -57.450         & \textbf{-56.355} & -56.610           & \textbf{-56.371}  & -56.706          \\
          \textbf{Accidents}  & -30.490           & -29.342        & -29.265         & \textbf{-26.982} & -28.510           & -28.351           & -29.692          \\
          \textbf{Retail}     & -11.029           & -10.944        & 10.942          & \textbf{-10.846} & -10.858           & -10.858           & \textbf{-10.836} \\
          \textbf{Pumsb-star} & -24.743           & -23.315        & -23.077         & \textbf{-22.405} & -22.866           & -22.664           & -23.702          \\
          \textbf{DNA}        & -80.982           & -81.913        & -81.840         & -81.211          & -80.730           & \textbf{-80.068}  & -85.568          \\
          \textbf{Kosarek}    & -10.894           & -10.719        & -10.685         & -10.599          & -10.690           & \textbf{-10.578}  & -10.615          \\
          \textbf{MSWeb}      & -10.108           & -9.833         & -9.838          & -9.726           & -9.630            & \textbf{-9.614}   & -9.819           \\
          \textbf{Book}       & -34.969           & -34.306        & -34.280         & -34.136          & -34.366           & \textbf{-33.818}  & -34.694          \\
          \textbf{EachMovie}  & -52.615           & -51.368        & -51.388         & -51.512          & \textbf{-50.263}  & \textbf{-50.414}  & -54.513          \\
          \textbf{WebKB}      & -158.164          & -154.283       & -153.911        & -151.838         & -151.341          & \textbf{-149.851} & -157.001         \\
          \textbf{Reuters-52} & -85.414           & -83.349        & -83.361         & -83.346          & \textbf{-81.544}  & -81.587           & -86.531          \\
          % \textbf{20 NewsG}  & -155.218          & -152.846       & -153.072        & -151.467         &                   &                   & -154.367         \\
          \textbf{BBC}        & -249.466          & -247.301       & -247.254        & -248.929         & \textbf{-226.359} & \textbf{-226.560} & -259.962         \\
          \textbf{Ad}         & -19.760           & -16.234        & -15.885         & -19.053          & -13.785           & \textbf{-13.595}  & -16.012          \\
          \bottomrule
        \end{tabular}
        \caption[Experimentation results]{\scriptsize Average test
          log likelihoods for all algorithms. In bold the best values
          after a Wilcoxon signed rank test with $p$-value of 0.05.}
        \label{tab:resexp}
      \end{table}   



      
      \end{textblock}


        
  % 
  % section 4
  \begin{textblock}{80}(0, 65.9)
    \usebeamerfont{section name}
    Regularizing by introducing tree distributions as leaves
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 68.4)
    \footnotesize
    \blindtext
    
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 68.4)
    \footnotesize
    By increasingly limiting the max number of allowed splits the depth of the
    structures increases. It is also worth noting how the size of the
    network decreases. 
    Other
    
    \begin{table}[ht]
      \setlength{\tabcolsep}{30pt}  
      \centering
      \begin{tabular}{c c}
        \includegraphics[width=0.4\linewidth]{figures/ll-depth/pumsb-star-ll-depth}&\includegraphics[width=0.4\linewidth]{figures/ll-m/pumsb-star-ll-m}
      \end{tabular}
    \end{table}
  \end{textblock}
  
  % \begin{textblock}{25.7}(54.2, 68.4)
  %   \small
  %   \blindtext
  % \end{textblock}
  
  
  
  % 
  % section 5
  \begin{textblock}{80}(0, 84.6)
    \usebeamerfont{section name}
    Strengthening by model averaging
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 87.1)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 87.1)
    \small
    \blindtext
  \end{textblock}
  
  % \begin{textblock}{25.7}(54.2, 87.1)
  %   \small
  %   \blindtext
  % \end{textblock}
  
  
  
  % 
  % section 5
  \begin{textblock}{80}(0, 103.3)
    \usebeamerfont{section name}
    References
  \end{textblock}
  
  % \begin{textblock}{25.7}(0, 105.8)
  %   \small
  %   % \blindtext
  %   \setlength\bibitemsep{8pt}
  %   \printbibliography
  % \end{textblock}

  \begin{textblock}{52}(0, 105.8)
    \small
    % \blindtext
    \setlength\bibitemsep{8pt}
    \printbibliography
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 105.8)
    \small
    % \blindtext
  \end{textblock}
  
  % \begin{textblock}{25.7}(54.2, 105.8)
  %   \small
  %   % \blindtext
  % \end{textblock}

  % 
  % footer
  \begin{textblock}{80}(0, 114.3)
    \usebeamerfont{subtitle}
    \textbf{ECML-PKDD 2015} - 8th September 2015, Porto, Portugal\hfill
    {\url{http://www.di.uniba.it/~vergari/code/spyn.html}}
  \end{textblock}
  
\end{frame}




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
