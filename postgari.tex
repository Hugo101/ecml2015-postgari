\documentclass[final]{beamer}

\usetheme{enziteto}

\usepackage[orientation=portrait, size=a0, scale=1.4, debug]{beamerposter}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[absolute,showboxes,overlay]{textpos}
\usepackage{calc}
\usepackage[colorgrid,texcoord]{eso-pic}
\usepackage{biblatex}
% \usepackage{enumitem}

\addbibresource{../referomnia/referomnia.bib}

\usepackage{blindtext}


\newcommand*\mccol[2]{\multicolumn{#1}{c}{#2}}
\newcommand*\tmccol[2]{\mccol{#1}{\tiny\textsf{#2}}}
\newcommand*\bmccol[2]{\mccol{#1}{\textbf{#2}}}


% \usepackage{xparse}
% \ExplSyntaxOn
% \NewDocumentCommand{\convertto}{mm}
% % #1 = em or ex (or any other unit)
% % #2 = dimen to convert
% {
%   \texttt{#2~=~\fp_to_decimal:n { round ( #2/1#1, 5 ) }#1}
% }
% \DeclareExpandableDocumentCommand{\thelength}{ O{mm} m }
% {
%   \fp_to_decimal:n { round ( #2/1#1, 5 ) } #1
% }
% \ExplSyntaxOff


%
% debugging colors
\definecolor{header}{RGB}{200, 125, 25}
\definecolor{footer}{RGB}{0, 153, 51}

\setbeamertemplate{itemize item}{\raisebox{.21ex}{\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\oplus}$}}\hspace{0pt}}}
\setbeamertemplate{itemize subitem}{\raise .2ex\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\otimes}$}}\hspace{0pt}}
\setbeamertemplate{itemize subsubitem}{\textcolor{lacamlilac}{$\oplus$}}
\setbeamertemplate{bibliography item}{\hspace{10pt}\raise .2ex\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\oplus}$}}}


% \setbeamertemplate{headline}{}

% \addbibresource{../referomnia/referomnia.bib}

\title{Simplifying, Regularizing and Strengthening Sum-Product Network Structure Learning}
\author{Antonio  Vergari, Nicola  {Di Mauro} and Floriana Esposito}
\date{}


\begin{document}

\institute{Universit√† degli Studi di Bari}
\department{Dipartimento di Informatica}
\laboratory{LACAM}
\group{Machine Learning}
\institutelogo{\includegraphics[width=25pt]{figures/unibaba}}
\lablogo{\includegraphics[width=35pt]{figures/lacam}}


% {
%   \setbeamertemplate{headline}{}
%   \setbeamertemplate{footline}{}
%   \begin{textblock}
%     \titlepage
%   \end{textblock}
% }

\newcommand{\hmargin}{20mm}
\newcommand{\vmargin}{20mm}
\textblockorigin{\hmargin}{\vmargin}

\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}

%
% TODO: generalize this
\newlength{\posterwidth}
\setlength{\posterwidth}{841mm - 2\hmargin}
\newlength{\posterheight}
\setlength{\posterheight}{1189mm}

\newcommand{\ncols}{3}
\newlength{\colwidth}
\setlength{\colwidth}{\posterwidth/\ncols}


\newlength{\colhpoint}


\begin{frame}{}
  %
  % title
  % \textblockcolour{header}
  \begin{textblock}{58}(0, 0)
    \usebeamerfont{section name}
    \huge
    Simplifying, Regularizing and Strengthening\\
    Sum-Product Network Structure Learning
  \end{textblock}
  %
  % authors
  \begin{textblock}{30}(0, 6.5)
    \usebeamerfont{author}
    \small
    Antonio  Vergari, Nicola  {Di Mauro} and Floriana Esposito
  \end{textblock}
  % 
  % email
  \begin{textblock}{15}(30, 6.5)
    \usebeamerfont{author}
    \small
    \emph{\{firstname.lastname@uniba.it\}}
  \end{textblock}
  %
  % affiliations
  \begin{textblock}{20}(60, 0)
    \usebeamerfont{author}
    \scriptsize
    \begin{minipage}[t]{5cm}
      \vspace{0pt}\hspace{10pt}
      \includegraphics[width=90pt]{figures/unibaba}
    \end{minipage}
    \begin{minipage}[t]{12cm}
    \vspace{27pt}
      \flushleft
      University of Bari "Aldo Moro", Italy\\
    \vspace{2pt}
      Department of Computer Science
    \end{minipage}\\[0.75cm]
    \usebeamerfont{author}
    \scriptsize
    \begin{minipage}[t]{5cm}
      \vspace{0pt}
      \includegraphics[width=110pt]{figures/lacam}
    \end{minipage}
    \begin{minipage}[t]{12cm}
      \vspace{23pt}
      \flushleft
      LACAM\\
      \vspace{2pt}
      Machine Learning
    \end{minipage}
  \end{textblock}
  
  
  %
  % section 1
  \begin{textblock}{80}(0, 9.8)
    \usebeamerfont{section name}
    Sum-Product Networks and Tractable Models
  \end{textblock}
  
  
  \begin{textblock}{25.7}(0, 12.3)
    \small
    Tractable inference on Probabilistic Graphical Models (PGMs) is
    at a trade off with model expressiveness.
    \begin{table}[!ht]
      %\setlength{\tabcolsep}{35pt}
      \centering
      \begin{tabular}{c c c c}
        
        \includegraphics[width=0.22\linewidth]{figures/mrf} &
        \includegraphics[width=0.22\linewidth]{figures/bn} &
        \includegraphics[width=0.22\linewidth]{figures/clt} &
        \includegraphics[width=0.22\linewidth]{figures/nf}\\                                                             
        \addlinespace[-0.2cm]
        % \scriptsize  $P(\mathbf{X})=\frac{1}{Z}\prod_{c}\phi_{c}(\mathbf{X}_{c})$ & 
        % \scriptsize  $P(\mathbf{X})=\prod_{i=1}^nP(X_{i}|\mathbf{Pa}_{i})$&
        %                                                                  %           \addlinespace[0.5cm]
                                                                            
        %                                                                     %\addlinespace[-0.2cm]
        % \scriptsize $P(\mathbf{X})=\prod_{i=1}^nP(X_{i}|Pa_{i})$ &
        % \scriptsize $P(\mathbf{X})=\prod_{i=1}^nP(X_{i})$ \\                                                             
        \scriptsize  $\frac{1}{Z}\prod_{c}\phi_{c}(\mathbf{X}_{c})$ & 
        \scriptsize  $\prod_{i=1}^nP(X_{i}|\mathbf{Pa}_{i})$&
        % \addlinespace[0.5cm]
         \scriptsize $\prod_{i=1}^nP(X_{i}|Pa_{i})$ &
        \scriptsize $\prod_{i=1}^nP(X_{i})$                                                              
      \end{tabular}
    \end{table}
    
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 12.3)
    \small
    \emph{Compiling} the partition function of a pdf into a \textbf{\emph{deep}} architecture of \textbf{sum}
    and \textbf{product} nodes.\\
    
    Product nodes define factorizations over independent vars, sum
    nodes mixtures. Leaves are tractable univariate distributions.\\
    
    % Sum node children weights are the parameters of the model.\\
    
    Products over nodes with different scopes (\emph{decomposability}) and
    sums over nodes with same scopes (\emph{completeness}) guarantee modeling
    a pdf (\emph{validity}).\\
    
    Considering only valid SPNs of \emph{alternated layers of sum and
      products}.
    
    \begin{minipage}{0.45\linewidth}
      \centering
      \includegraphics[width=0.8\linewidth]{figures/spn-prod}
    \end{minipage}\begin{minipage}{0.45\linewidth}
      \centering
      \includegraphics[width=0.7\linewidth]{figures/spn-sum}
    \end{minipage}
    
    
  \end{textblock}

  % \begin{textblock}{25.7}(34.1, 15.3)
  %   \small
  %     \includegraphics[width=0.7\linewidth]{figures/spn-sum}
  % \end{textblock}
  
  \begin{textblock}{25.7}(54.2, 12.3)
    \small
    \emph{Compiling} the partition function of a pdf into a \textbf{\emph{deep}} architecture of \textbf{sum}
    and \textbf{product} nodes.\\
    
    Product nodes define factorizations over independent vars, sum
    nodes mixtures. Leaves are tractable univariate distributions.\\
    
    % Sum node children weights are the parameters of the model.\\
    
    Products over nodes with different scopes (\emph{decomposability}) and
    sums over nodes with same scopes (\emph{completeness}) guarantee modeling
    a pdf (\emph{validity}).\\
    
    Considering only valid SPNs of \emph{alternated layers of sum and
      products}.
    
    \begin{minipage}{0.45\linewidth}
      \centering
      \includegraphics[width=0.8\linewidth]{figures/spn-prod}
    \end{minipage}\begin{minipage}{0.45\linewidth}
      \centering
      \includegraphics[width=0.7\linewidth]{figures/spn-sum}
x    \end{minipage}


  \end{textblock}
  
  %
  % section 2
  \begin{textblock}{80}(0, 28.5)
    \usebeamerfont{section name}
    How and why to perform structure learning
    
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 31.)
    \small
    Fixed structures are hard to engineer and train (fully connected
    layers). Structure learning is more flexible and enables automatic
    latent features discovery.\par
    
    Constraint-based search formulation. Discover hidden variables for sum node mixtures and independences
    for product node components:
    \begin{itemize}
      \itemsep 6pt
    \item greedy top-down: KMeans on features~\emph{\parencite{Dennis2012}}; alternating clustering on
      instances and independence tests on features, \textbf{LearnSPN}~\emph{\parencite{Gens2013}}
      
    \item greedy bottom up: merging feature regions by a \emph{Bayesian-Dirichlet independence test},  and reducing edges by maximizing MI\emph{~\parencite{Peharz2013}}

    \item \textbf{ID-SPN}: turning LearnSPN in log-likelihood guided expansion of sub-networks
      approximated by Arithmetic Circuits~\emph{\parencite{Rooshenas2014}}
      
    \end{itemize}
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 31.)
    \footnotesize
    LearnSPN~\parencite{Gens2013} builds a tree-like SPN by recursively split the data matrix:

    \begin{itemize}
    \item splitting columns in pairs by a greedy \textbf{\emph{G Test}} based
      procedure with threshold $\rho$:
      \[
      G(X_i, X_j) =  2\sum_{x_i \sim X_i}\sum_{x_j \sim X_j}c(x_i, x_j)\cdot \log\frac{c(x_i, x_j)\cdot |T|}{c(x_i)c(x_j)}
      \]
 
      \end{itemize}
  \end{textblock}
  
  
  \begin{textblock}{25.7}(33.1, 36.7)
    \small
    \begin{minipage}[t][][t]{6.3cm}
      \includegraphics[width=\linewidth]{figures/grid-0}
    \end{minipage}\begin{minipage}[t]{5.54cm}
      \includegraphics[width=\linewidth]{figures/grid-1}
    \end{minipage}\raisebox{120pt}{\begin{minipage}[t]{6.3cm}\vspace{-120pt}
      \includegraphics[width=\linewidth]{figures/learnspn-1}
    \end{minipage}}\begin{minipage}[t]{5.54cm}
      \includegraphics[width=\linewidth]{figures/grid-2}
    \end{minipage}\raisebox{42pt}{\begin{minipage}[t]{7.2cm}
      \includegraphics[width=\linewidth]{figures/learnspn-2}
    \end{minipage}}\begin{minipage}[t]{5.724cm}
    \includegraphics[width=\linewidth]{figures/grid-3}                                                           \end{minipage}\raisebox{42pt}{\begin{minipage}[t]{9cm}
      \includegraphics[width=\linewidth]{figures/learnspn-3}                                             
    \end{minipage}}
    
    % \begin{table}[ht]
    %   \centering
    %   \begin{tabular}{l l l l l l l}
    %     \includegraphics[width=0.228\linewidth]{figures/grid-0}&
    %     \includegraphics[width=0.2\linewidth]{figures/grid-1}&
    %     \includegraphics[width=0.2\linewidth]{figures/learnspn-1}&                                                       \includegraphics[width=0.2\linewidth]{figures/grid-2}&
    %     \includegraphics[width=0.24\linewidth]{figures/learnspn-2}&
    %     \includegraphics[width=0.208\linewidth]{figures/grid-3}&                                                          \includegraphics[width=0.30\linewidth]{figures/learnspn-3}\\                                             
    %   \end{tabular}
    % \end{table}
    
  \end{textblock}
  
  
  \begin{textblock}{25.7}(54.2, 31.)
    \footnotesize
    \begin{itemize}
  \item clustering instances with \textbf{\emph{online Hard-EM}} with cluster penalty
    $\lambda$:
    \[\begin{array}{cc}
        Pr(\mathbf{X})= \sum_{C_i \in \mathbf{C}}\prod_{X_j \in \mathbf{X}}Pr(X_j|C_i)Pr(C_i)\\
        % & Pr(C_i) \propto e^{-\lambda |\mathbf{C}|\cdot |\mathbf{X}|}\\
      \end{array}\]
    \item if there are less than $m$ instances, put a \textbf{\emph{naive
          factorization}} over leaves
    \item each univariate distribution get \emph{\textbf{ML estimation}} smoothed by $\alpha$  
    \end{itemize}
  \end{textblock}
  
  
  %
  % section 3
  \begin{textblock}{80}(0, 47.2)
    \usebeamerfont{section name}
    Simplifying by limiting node splits
  \end{textblock}

  % 
  % section 3.1
  \begin{textblock}{20}(54.2, 47.2)
    \usebeamerfont{section name}
    Experiments
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 49.7)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 49.7)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(54.2, 49.7)
    \footnotesize
    Classical setting for \emph{\textbf{generative}} graphical models
    structure learning \parencite{Gens2013}:
    \setlength{\leftmargini}{30pt}
    \begin{itemize}
      \itemsep 7pt
    \item 19 binary datasets from classification, recommendation,
      frequent pattern mining\dots \parencite{Lowd2010} \parencite{Haaren2012}
    \item Training 75\% Validation 10\% Test 15\%  splits (no cv)
    \end{itemize}\bigskip

    Comparing both accuracy and structure quality:
    \begin{itemize}
    \item \emph{\textbf{average log-likelihood}} on predicting test
      instances
    \item networks sizes (\# edges)
    \item network depth (\# alternated type layers)
      \item latent interactions captured (\# number of parameters)

    \end{itemize}\bigskip
    
    Comparing the state-of-the-art, \textsf{LearnSPN},
    \textsf{ID-SPN} and \textsf{MT}~\parencite{Meila2000}, against our
    variations:
    
    \begin{itemize}
    \item \textsf{SPN-B} using only \textsf{B}inary splits
    \item \textsf{SPN-BT} with \textsf{B}inary splits and \textsf{T}rees as leaves
    \item \textsf{SPN-BB} combining \textsf{B}inary splits and \textsf{B}agging
      \item \textsf{SPN-BTB} including all variants
    \end{itemize}\bigskip
    
    Model selection via \textit{grid search} in the same parameter space:
      \begin{itemize}
      \item $\lambda \in \{0.2, 0.4, 0.6, 0.8\}$,
      \item $\rho \in \{5, 10, 15, 20\}$, 
      \item $m \in \{1, 50, 100, 500\}$, 
      \item $\alpha \in \{ 0.1, 0.2, 0.5, 1.0, 2.0\}$
      \end{itemize}


      \begin{table}[!htbp]
        \centering
        \tiny
        \setlength{\tabcolsep}{3pt}  
        \begin{tabular}{r r r r l r r r r r r}
          \toprule
          & \bmccol{4}{\# edges}    & \bmccol{3}{\# layers} & \bmccol{3}{\# params}                                                                                                                                                   \\
          \midrule
          % & \textsf{LearnSPN}       & \textsf{SPN-B}
          % & SPN-BT}                 & LearnSPN}             & SPN-B}                & SPN-BT}                 & LearnSPN}            & SPN-B}                & SPN-BT}                                                                \\
          & \tmccol{1}{LearnSPN}    & \tmccol{1}{SPN-B}     & \tmccol{2}{SPN-BT}    & \tmccol{1}{LearnSPN}    & \tmccol{1}{SPN-B}    & \tmccol{1}{SPN-BT}    & \tmccol{1}{LearnSPN}    & \tmccol{1}{SPN-B}    & \tmccol{1}{SPN-BT}    \\
          % & \rot{\textsf{LearnSPN}} & \rot{\textsf{SPN-B}}  & \rot{\textsf{SPN-BT}} & \rot{\textsf{LearnSPN}} & \rot{\textsf{SPN-B}} & \rot{\textsf{SPN-BT}} & \rot{\textsf{LearnSPN}} & \rot{\textsf{SPN-B}} & \rot{\textsf{SPN-BT}} \\
          % & e                       & l                     & p                     & e                       & l                    & p                     & e                       & l                    & p                     \\
          \midrule       
          \textbf{NLTCS}      & 7509                    & 1133                  & 1133                  & (1125)                  & 4                    & 15                    & 15                      & 476                  & 275  & 275            \\
          \textbf{MSNBC}      & 22350                   & 4258                  & 4258                  & (3996)                  & 4                    & 21                    & 21                      & 1680                 & 1071 & 1071           \\
          \textbf{KDDCup2k}   & 44544                   & 4272                  & 4272                  & (4166)                  & 4                    & 25                    & 25                      & 753                  & 760  & 760            \\
          \textbf{Plants}     & 55668                   & 13720                 & 5948                  & (1840)                  & 6                    & 23                    & 20                      & 3819                 & 2397 & 490            \\
          \textbf{Audio}      & 70036                   & 16421                 & 4059                  & (478)                   & 8                    & 23                    & 15                      & 3389                 & 2631 & 105            \\
          \textbf{Jester}     & 36528                   & 10793                 & 10793                 & (8587)                  & 4                    & 19                    & 19                      & 563                  & 1932 & 1932           \\
          \textbf{Netflix}    & 17742                   & 25009                 & 4132                  & (203)                   & 4                    & 25                    & 14                      & 1499                 & 4070 & 82             \\
          \textbf{Accidents}  & 48654                   & 12367                 & 10547                 & (6687)                  & 6                    & 25                    & 26                      & 5390                 & 2708 & 1977           \\
          \textbf{Retail}     & 7487                    & 1188                  & 1188                  & (1153)                  & 4                    & 23                    & 23                      & 171                  & 224  & 224            \\
          \textbf{Pumsb-star} & 15247                   & 12800                 & 9984                  & (6175)                  & 8                    & 25                    & 23                      & 1911                 & 2662 & 1680           \\
          \textbf{DNA}        & 17602                   & 3178                  & 4225                  & (2746)                  & 6                    & 13                    & 12                      & 947                  & 884  & 1113           \\
          \textbf{Kosarek}    & 7993                    & 8174                  & 2216                  & (1311)                  & 6                    & 27                    & 21                      & 781                  & 1462 & 242            \\
          \textbf{MSWeb}      & 17339                   & 9116                  & 7568                  & (6797)                  & 6                    & 27                    & 34                      & 620                  & 1672 & 1446           \\
          \textbf{Book}       & 42491                   & 9917                  & 3503                  & (3485)                  & 4                    & 15                    & 13                      & 1176                 & 1351 & 430            \\
          \textbf{EachMovie}  & 52693                   & 20756                 & 20756                 & (17861)                 & 8                    & 23                    & 23                      & 1010                 & 2637 & 2637           \\
          \textbf{WebKB}      & 52498                   & 45620                 & 8796                  & (6874)                  & 8                    & 23                    & 16                      & 1712                 & 6087 & 1128           \\
          \textbf{Reuters-52} & 307113                  & 77336                 & 77336                 & (59197)                 & 12                   & 31                    & 31                      & 3641                 & 8968 & 8968           \\
          % \textbf{20 NewsG} & 438100                  & 169326                & (114951)              & 10                      & 31                   & 33                    & 3442                    & 18117                & 18065                 \\
          \textbf{BBC}        & 318313                  & 63723                 & 63723                 & (41247)                 & 16                   & 27                    & 27                      & 1134                 & 6147 & 6147           \\
          \textbf{Ad}         & 70056                   & 23606                 & 23606                 & (20079)                 & 16                   & 59                    & 59                      & 1060                 & 1222 & 1222           \\
          \bottomrule
        \end{tabular}
        \caption[Experimentation statistics]{\scriptsize Structural quality results for
          the best validation models for \textsf{LearnSPN},
          \textsf{SPN-B} and \textsf{SPN-BT} as the number
          of edges, layers and parameters. For \textsf{SPN-BT} are reported
          the number of edges considering those in the Chow-Liu leaves
          and without considering them (in parenthesis).}
        \label{tab:expstats}
      \end{table}



      \begin{table}[!htbp]
        \centering
        \tiny
        \setlength{\tabcolsep}{3pt}  
        \begin{tabular}{r r r r r r r r}
          \toprule
          & \textsf{LearnSPN} & \textsf{SPN-B} & \textsf{SPN-BT} & \textsf{ID-SPN}  & \textsf{SPN-BB}   & \textsf{SPN-BTB}  & \textsf{MT}      \\
          \midrule                                                                                     
          \textbf{NLTCS}      & -6.110            & -6.048         & -6.048          & \textbf{-5.998}  & -6.014            & -6.014            & -6.008           \\
          \textbf{MSNBC}      & -6.099            & -6.040         & -6.039          & -6.040           & \textbf{-6.032}   & -6.033            & -6.076           \\
          \textbf{KDDCup2k}   & -2.185            & -2.141         & -2.141          & -2.134           & -2.122            & \textbf{-2.121}   & -2.135           \\
          \textbf{Plants}     & -12.878           & -12.813        & -12.683         & -12.537          & -12.167           & \textbf{-12.089}  & -12.926          \\
          \textbf{Audio}      & -40.360           & -40.571        & -40.484         & -39.794          & -39.685           & \textbf{-39.616}  & -40.142          \\
          \textbf{Jester}     & -53.300           & -53.537        & -53.546         & \textbf{-52.858} & \textbf{-52.873}  & -53.600           & -53.057          \\
          \textbf{Netflix}    & -57.191           & -57.730        & -57.450         & \textbf{-56.355} & -56.610           & \textbf{-56.371}  & -56.706          \\
          \textbf{Accidents}  & -30.490           & -29.342        & -29.265         & \textbf{-26.982} & -28.510           & -28.351           & -29.692          \\
          \textbf{Retail}     & -11.029           & -10.944        & 10.942          & \textbf{-10.846} & -10.858           & -10.858           & \textbf{-10.836} \\
          \textbf{Pumsb-star} & -24.743           & -23.315        & -23.077         & \textbf{-22.405} & -22.866           & -22.664           & -23.702          \\
          \textbf{DNA}        & -80.982           & -81.913        & -81.840         & -81.211          & -80.730           & \textbf{-80.068}  & -85.568          \\
          \textbf{Kosarek}    & -10.894           & -10.719        & -10.685         & -10.599          & -10.690           & \textbf{-10.578}  & -10.615          \\
          \textbf{MSWeb}      & -10.108           & -9.833         & -9.838          & -9.726           & -9.630            & \textbf{-9.614}   & -9.819           \\
          \textbf{Book}       & -34.969           & -34.306        & -34.280         & -34.136          & -34.366           & \textbf{-33.818}  & -34.694          \\
          \textbf{EachMovie}  & -52.615           & -51.368        & -51.388         & -51.512          & \textbf{-50.263}  & \textbf{-50.414}  & -54.513          \\
          \textbf{WebKB}      & -158.164          & -154.283       & -153.911        & -151.838         & -151.341          & \textbf{-149.851} & -157.001         \\
          \textbf{Reuters-52} & -85.414           & -83.349        & -83.361         & -83.346          & \textbf{-81.544}  & -81.587           & -86.531          \\
          % \textbf{20 NewsG}  & -155.218          & -152.846       & -153.072        & -151.467         &                   &                   & -154.367         \\
          \textbf{BBC}        & -249.466          & -247.301       & -247.254        & -248.929         & \textbf{-226.359} & \textbf{-226.560} & -259.962         \\
          \textbf{Ad}         & -19.760           & -16.234        & -15.885         & -19.053          & -13.785           & \textbf{-13.595}  & -16.012          \\
          \bottomrule
        \end{tabular}
        \caption[Experimentation results]{\scriptsize Average test
          log likelihoods for all algorithms.}
        \label{tab:resexp}
      \end{table}   



      
      \end{textblock}


        
  % 
  % section 4
  \begin{textblock}{80}(0, 65.9)
    \usebeamerfont{section name}
    Regularizing by introducing tree distributions as leaves
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 68.4)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 68.4)
    \small
    \blindtext
  \end{textblock}
  
  % \begin{textblock}{25.7}(54.2, 68.4)
  %   \small
  %   \blindtext
  % \end{textblock}
  
  
  
  % 
  % section 5
  \begin{textblock}{80}(0, 84.6)
    \usebeamerfont{section name}
    Strengthening by model averaging
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 87.1)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 87.1)
    \small
    \blindtext
  \end{textblock}
  
  % \begin{textblock}{25.7}(54.2, 87.1)
  %   \small
  %   \blindtext
  % \end{textblock}
  
  
  
  % 
  % section 5
  \begin{textblock}{80}(0, 103.3)
    \usebeamerfont{section name}
    References
  \end{textblock}
  
  % \begin{textblock}{25.7}(0, 105.8)
  %   \small
  %   % \blindtext
  %   \setlength\bibitemsep{8pt}
  %   \printbibliography
  % \end{textblock}

  \begin{textblock}{52}(0, 105.8)
    \small
    % \blindtext
    \setlength\bibitemsep{8pt}
    \printbibliography
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 105.8)
    \small
    % \blindtext
  \end{textblock}
  
  % \begin{textblock}{25.7}(54.2, 105.8)
  %   \small
  %   % \blindtext
  % \end{textblock}

  % 
  % footer
  \begin{textblock}{80}(0, 114.3)
    \usebeamerfont{subtitle}
    \textbf{ECML-PKDD 2015} - 8th September 2015, Porto, Portugal\hfill
    {\url{http://www.di.uniba.it/~vergari/code/spyn.html}}
  \end{textblock}
  
\end{frame}




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
