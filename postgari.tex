\documentclass[final]{beamer}

\usetheme{enziteto}

\usepackage[orientation=portrait, size=a0, scale=1.4, debug]{beamerposter}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[absolute,showboxes,overlay]{textpos}
\usepackage{calc}
\usepackage[colorgrid,texcoord]{eso-pic}
\usepackage{biblatex}

\addbibresource{../referomnia/referomnia.bib}

\usepackage{blindtext}


% \usepackage{xparse}
% \ExplSyntaxOn
% \NewDocumentCommand{\convertto}{mm}
% % #1 = em or ex (or any other unit)
% % #2 = dimen to convert
% {
%   \texttt{#2~=~\fp_to_decimal:n { round ( #2/1#1, 5 ) }#1}
% }
% \DeclareExpandableDocumentCommand{\thelength}{ O{mm} m }
% {
%   \fp_to_decimal:n { round ( #2/1#1, 5 ) } #1
% }
% \ExplSyntaxOff


%
% debugging colors
\definecolor{header}{RGB}{200, 125, 25}
\definecolor{footer}{RGB}{0, 153, 51}

\setbeamertemplate{itemize item}{\raisebox{.21ex}{\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\oplus}$}}\hspace{-2pt}}}
\setbeamertemplate{itemize subitem}{\raise .2ex\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\otimes}$}}\hspace{-3pt}}
\setbeamertemplate{itemize subsubitem}{\textcolor{lacamlilac}{$\oplus$}}
\setbeamertemplate{bibliography item}{\hspace{10pt}\raise .2ex\hbox{\tiny\textcolor{lacamlilac}{$\boldsymbol{\oplus}$}}}


% \setbeamertemplate{headline}{}

% \addbibresource{../referomnia/referomnia.bib}

\title{Simplifying, Regularizing and Strengthening Sum-Product Network Structure Learning}
\author{Antonio  Vergari, Nicola  {Di Mauro} and Floriana Esposito}
\date{}


\begin{document}

\institute{Universit√† degli Studi di Bari}
\department{Dipartimento di Informatica}
\laboratory{LACAM}
\group{Machine Learning}
\institutelogo{\includegraphics[width=25pt]{figures/unibaba}}
\lablogo{\includegraphics[width=35pt]{figures/lacam}}


% {
%   \setbeamertemplate{headline}{}
%   \setbeamertemplate{footline}{}
%   \begin{textblock}
%     \titlepage
%   \end{textblock}
% }

\newcommand{\hmargin}{20mm}
\newcommand{\vmargin}{20mm}
\textblockorigin{\hmargin}{\vmargin}

\setlength{\TPHorizModule}{1cm}
\setlength{\TPVertModule}{1cm}

%
% TODO: generalize this
\newlength{\posterwidth}
\setlength{\posterwidth}{841mm - 2\hmargin}
\newlength{\posterheight}
\setlength{\posterheight}{1189mm}

\newcommand{\ncols}{3}
\newlength{\colwidth}
\setlength{\colwidth}{\posterwidth/\ncols}


\newlength{\colhpoint}


\begin{frame}{}
  %
  % title
  % \textblockcolour{header}
  \begin{textblock}{58}(0, 0)
    \usebeamerfont{section name}
    \huge
    Simplifying, Regularizing and Strengthening\\
    Sum-Product Network Structure Learning
  \end{textblock}
  %
  % authors
  \begin{textblock}{30}(0, 6.5)
    \usebeamerfont{author}
    \small
    Antonio  Vergari, Nicola  {Di Mauro} and Floriana Esposito
  \end{textblock}
  % 
  % email
  \begin{textblock}{15}(30, 6.5)
    \usebeamerfont{author}
    \small
    \emph{\{firstname.lastname@uniba.it\}}
  \end{textblock}
  %
  % affiliations
  \begin{textblock}{20}(60, 0)
    \usebeamerfont{author}
    \scriptsize
    \begin{minipage}[t]{5cm}
      \vspace{0pt}\hspace{10pt}
      \includegraphics[width=90pt]{figures/unibaba}
    \end{minipage}
    \begin{minipage}[t]{12cm}
    \vspace{27pt}
      \flushleft
      University of Bari "Aldo Moro", Italy\\
    \vspace{2pt}
      Department of Computer Science
    \end{minipage}\\[0.75cm]
    \usebeamerfont{author}
    \scriptsize
    \begin{minipage}[t]{5cm}
      \vspace{0pt}
      \includegraphics[width=110pt]{figures/lacam}
    \end{minipage}
    \begin{minipage}[t]{12cm}
      \vspace{23pt}
      \flushleft
      LACAM\\
      \vspace{2pt}
      Machine Learning
    \end{minipage}
  \end{textblock}
  
  
  %
  % section 1
  \begin{textblock}{80}(0, 9.8)
    \usebeamerfont{section name}
    Sum-Product Networks and Tractable Models
  \end{textblock}
  
  
  \begin{textblock}{25.7}(0, 12.3)
    \small
    Tractable inference on Probabilistic Graphical Models (PGMs) is
    at a trade off with model expressiveness.
    \begin{table}[!ht]
      %\setlength{\tabcolsep}{35pt}
      \centering
      \begin{tabular}{c c c c}
        
        \includegraphics[width=0.22\linewidth]{figures/mrf} &
        \includegraphics[width=0.22\linewidth]{figures/bn} &
        \includegraphics[width=0.22\linewidth]{figures/clt} &
        \includegraphics[width=0.22\linewidth]{figures/nf}\\                                                             
        \addlinespace[-0.2cm]
        % \scriptsize  $P(\mathbf{X})=\frac{1}{Z}\prod_{c}\phi_{c}(\mathbf{X}_{c})$ & 
        % \scriptsize  $P(\mathbf{X})=\prod_{i=1}^nP(X_{i}|\mathbf{Pa}_{i})$&
        %                                                                  %           \addlinespace[0.5cm]
                                                                            
        %                                                                     %\addlinespace[-0.2cm]
        % \scriptsize $P(\mathbf{X})=\prod_{i=1}^nP(X_{i}|Pa_{i})$ &
        % \scriptsize $P(\mathbf{X})=\prod_{i=1}^nP(X_{i})$ \\                                                             
        \scriptsize  $\frac{1}{Z}\prod_{c}\phi_{c}(\mathbf{X}_{c})$ & 
        \scriptsize  $\prod_{i=1}^nP(X_{i}|\mathbf{Pa}_{i})$&
        % \addlinespace[0.5cm]
         \scriptsize $\prod_{i=1}^nP(X_{i}|Pa_{i})$ &
        \scriptsize $\prod_{i=1}^nP(X_{i})$                                                              
      \end{tabular}
    \end{table}
    
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 12.3)
    \small
    \emph{Compiling} the partition function of a pdf into a \textbf{\emph{deep}} architecture of \textbf{sum}
    and \textbf{product} nodes.\\
    
    Product nodes define factorizations over independent vars, sum
    nodes mixtures. Leaves are tractable univariate distributions.\\
    
    % Sum node children weights are the parameters of the model.\\
    
    Products over nodes with different scopes (\emph{decomposability}) and
    sums over nodes with same scopes (\emph{completeness}) guarantee modeling
    a pdf (\emph{validity}).\\
    
    Considering only valid SPNs of \emph{alternated layers of sum and
      products}.
    
    \begin{minipage}{0.45\linewidth}
      \centering
      \includegraphics[width=0.8\linewidth]{figures/spn-prod}
    \end{minipage}\begin{minipage}{0.45\linewidth}
      \centering
      \includegraphics[width=0.7\linewidth]{figures/spn-sum}
    \end{minipage}
    
    
  \end{textblock}

  % \begin{textblock}{25.7}(34.1, 15.3)
  %   \small
  %     \includegraphics[width=0.7\linewidth]{figures/spn-sum}
  % \end{textblock}
  
  \begin{textblock}{25.7}(54.2, 12.3)
    \small
    \emph{Compiling} the partition function of a pdf into a \textbf{\emph{deep}} architecture of \textbf{sum}
    and \textbf{product} nodes.\\
    
    Product nodes define factorizations over independent vars, sum
    nodes mixtures. Leaves are tractable univariate distributions.\\
    
    % Sum node children weights are the parameters of the model.\\
    
    Products over nodes with different scopes (\emph{decomposability}) and
    sums over nodes with same scopes (\emph{completeness}) guarantee modeling
    a pdf (\emph{validity}).\\
    
    Considering only valid SPNs of \emph{alternated layers of sum and
      products}.
    
    \begin{minipage}{0.45\linewidth}
      \centering
      \includegraphics[width=0.8\linewidth]{figures/spn-prod}
    \end{minipage}\begin{minipage}{0.45\linewidth}
      \centering
      \includegraphics[width=0.7\linewidth]{figures/spn-sum}
x    \end{minipage}


  \end{textblock}
  
  %
  % section 2
  \begin{textblock}{80}(0, 28.5)
    \usebeamerfont{section name}
    How and Why to perform Structure Learning
    
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 31.)
    \small
    Fixed structures are hard to engineer and train (fully connected
    layers). Structure learning is more flexible and enables automatic
    latent features discovery.\par
    
    Constraint-based search formulation. Discover hidden variables for sum node mixtures and independences
    for product node components:
    \begin{itemize}
      \itemsep 6pt
    \item greedy top-down: KMeans on features~\emph{\parencite{Dennis2012}}; alternating clustering on
      instances and independence tests on features, \textbf{LearnSPN}~\emph{\parencite{Gens2013}}
      
    \item greedy bottom up: merging feature regions by a \emph{Bayesian-Dirichlet independence test},  and reducing edges by maximizing MI\emph{~\parencite{Peharz2013}}

    \item \textbf{ID-SPN}: turning LearnSPN in log-likelihood guided expansion of sub-networks
      approximated by Arithmetic Circuits~\emph{\parencite{Rooshenas2014}}
      
    \end{itemize}
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 31.)
    \footnotesize
    LearnSPN~\parencite{Gens2013} builds a tree-like SPN by recursively split the data matrix:

    \begin{itemize}
    \item splitting columns in pairs by a greedy \textbf{\emph{G Test}} based
      procedure with threshold $\rho$:
      \[
      G(X_i, X_j) =  2\sum_{x_i \sim X_i}\sum_{x_j \sim X_j}c(x_i, x_j)\cdot \log\frac{c(x_i, x_j)\cdot |T|}{c(x_i)c(x_j)}
      \]
 
      \end{itemize}
  \end{textblock}
  
  
  \begin{textblock}{25.7}(33.1, 36.7)
    \small
    \begin{minipage}[t][][t]{6.3cm}
      \includegraphics[width=\linewidth]{figures/grid-0}
    \end{minipage}\begin{minipage}[t]{5.54cm}
      \includegraphics[width=\linewidth]{figures/grid-1}
    \end{minipage}\raisebox{120pt}{\begin{minipage}[t]{6.3cm}\vspace{-120pt}
      \includegraphics[width=\linewidth]{figures/learnspn-1}
    \end{minipage}}\begin{minipage}[t]{5.54cm}
      \includegraphics[width=\linewidth]{figures/grid-2}
    \end{minipage}\raisebox{42pt}{\begin{minipage}[t]{7.2cm}
      \includegraphics[width=\linewidth]{figures/learnspn-2}
    \end{minipage}}\begin{minipage}[t]{5.724cm}
    \includegraphics[width=\linewidth]{figures/grid-3}                                                           \end{minipage}\raisebox{42pt}{\begin{minipage}[t]{9cm}
      \includegraphics[width=\linewidth]{figures/learnspn-3}                                             
    \end{minipage}}
    
    % \begin{table}[ht]
    %   \centering
    %   \begin{tabular}{l l l l l l l}
    %     \includegraphics[width=0.228\linewidth]{figures/grid-0}&
    %     \includegraphics[width=0.2\linewidth]{figures/grid-1}&
    %     \includegraphics[width=0.2\linewidth]{figures/learnspn-1}&                                                       \includegraphics[width=0.2\linewidth]{figures/grid-2}&
    %     \includegraphics[width=0.24\linewidth]{figures/learnspn-2}&
    %     \includegraphics[width=0.208\linewidth]{figures/grid-3}&                                                          \includegraphics[width=0.30\linewidth]{figures/learnspn-3}\\                                             
    %   \end{tabular}
    % \end{table}
    
  \end{textblock}
  
  
  \begin{textblock}{25.7}(54.2, 31.)
    \footnotesize
    \begin{itemize}
  \item clustering instances with \textbf{\emph{online Hard-EM}} with cluster penalty
    $\lambda$:
    \[\begin{array}{cc}
        Pr(\mathbf{X})= \sum_{C_i \in \mathbf{C}}\prod_{X_j \in \mathbf{X}}Pr(X_j|C_i)Pr(C_i)\\
        % & Pr(C_i) \propto e^{-\lambda |\mathbf{C}|\cdot |\mathbf{X}|}\\
      \end{array}\]
    \item if there are less than $m$ instances, put a \textbf{\emph{naive
          factorization}} over leaves
    \item each univariate distribution get \emph{\textbf{ML estimation}} smoothed by $\alpha$  
    \end{itemize}
  \end{textblock}
  
  
  %
  % section 3
  \begin{textblock}{80}(0, 47.2)
    \usebeamerfont{section name}
    Simplifying by Limiting Node Splits
  \end{textblock}

  % 
  % section 3.1
  \begin{textblock}{20}(54.2, 47.2)
    \usebeamerfont{section name}
    Experiments
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 49.7)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 49.7)
    \small
    \blindtext
  \end{textblock}
  
  
  \begin{textblock}{25.7}(54.2, 49.7)
    \footnotesize
    Classical setting for \emph{\textbf{generative}} graphical models
    structure learning \parencite{Gens2013}:
    \begin{itemize}
      \itemsep 7pt
    \item comparing the \emph{\textbf{average log-likelihood}} on predicting instances from a test set
    \item 19 binary datasets from classification, recommendation,
      frequent pattern mining\dots \parencite{Lowd2010} \parencite{Haaren2012}
    \item Training 75\% Validation 10\% Test 15\%  splits (no cv)
    \item Model selection via \textit{grid search} in the same parameter space:
      \begin{itemize}
        \footnotesize
      \item $\lambda \in \{0.2, 0.4, 0.6, 0.8\}$,
      \item $\rho \in \{5, 10, 15, 20\}$, 
      \item $m \in \{1, 50, 100, 500\}$, 
      \item $\alpha \in \{ 0.1, 0.2, 0.5, 1.0, 2.0\}$
      \end{itemize}
    \item comparing our variants against \textsf{LearnSPN},
      \textsf{ID-SPN} and \textsf{MT}~\parencite{Meila2000}
    \end{itemize}  \end{textblock}
  
  
  % 
  % section 4
  \begin{textblock}{80}(0, 65.9)
    \usebeamerfont{section name}
    Regularizing by introducing tree distributions as leaves
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 68.4)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 68.4)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(54.2, 68.4)
    \small
    \blindtext
  \end{textblock}
  
  
  
  % 
  % section 5
  \begin{textblock}{80}(0, 84.6)
    \usebeamerfont{section name}
    Strengthening by Model Averaging
  \end{textblock}
  
  \begin{textblock}{25.7}(0, 87.1)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 87.1)
    \small
    \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(54.2, 87.1)
    \small
    \blindtext
  \end{textblock}
  
  
  
  % 
  % section 5
  \begin{textblock}{80}(0, 103.3)
    \usebeamerfont{section name}
    References
  \end{textblock}
  
  % \begin{textblock}{25.7}(0, 105.8)
  %   \small
  %   % \blindtext
  %   \setlength\bibitemsep{8pt}
  %   \printbibliography
  % \end{textblock}

  \begin{textblock}{52}(0, 105.8)
    \small
    % \blindtext
    \setlength\bibitemsep{8pt}
    \printbibliography
  \end{textblock}
  
  \begin{textblock}{25.7}(27.1, 105.8)
    \small
    % \blindtext
  \end{textblock}
  
  \begin{textblock}{25.7}(54.2, 105.8)
    \small
    % \blindtext
  \end{textblock}

  % 
  % footer
  \begin{textblock}{80}(0, 114.3)
    \usebeamerfont{subtitle}
    \textbf{ECML-PKDD 2015} - 8th September 2015, Porto, Portugal\hfill
    {\url{http://www.di.uniba.it/~vergari/code/spyn.html}}
  \end{textblock}
  
\end{frame}




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:
